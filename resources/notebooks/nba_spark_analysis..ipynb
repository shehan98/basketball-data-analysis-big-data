{
  "metadata": {
    "name": "nba_spark_analysis",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Import necessary libraries\nfrom pyspark.sql import SparkSession\n\n# Initialize Spark Session\nspark \u003d SparkSession.builder \\\n    .appName(\"NBA Analysis with Spark\") \\\n    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n    .config(\"spark.sql.catalogImplementation\", \"in-memory\") \\\n    .getOrCreate()\n\n# Define HDFS path\nhdfs_path \u003d \"hdfs://namenode:9000/user/test/output/cleanedData\"\n\n# Load data into DataFrame\ndf \u003d spark.read.csv(hdfs_path, header\u003dFalse, inferSchema\u003dFalse)\n\n# Assign column names to match the schema\ncolumn_names \u003d [\n    \"event_id\", \"event_num\", \"game_id\", \"home_description\", \"time\", \"period\",\n    \"player1_id\", \"player1_name\", \"player1_team_abbr\", \"player1_team_city\",\n    \"player1_team_id\", \"player1_team_name\", \"player2_id\", \"player2_name\",\n    \"player2_team_abbr\", \"player2_team_city\", \"player2_team_id\", \"player2_team_name\",\n    \"player3_id\", \"player3_name\", \"player3_team_abbr\", \"player3_team_city\",\n    \"player3_team_id\", \"player3_team_name\", \"score\", \"score_margin\", \"visitor_description\"\n]\ndf \u003d df.toDF(*column_names)"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql.functions import col, regexp_extract\nfrom pyspark.sql.functions import when, sum as _sum\n\n# Extract home and visitor scores from the \u0027score\u0027 column in the format \u0027X - Y\u0027\ndf \u003d df.withColumn(\"home_score\", regexp_extract(col(\"score\"), r\"(\\d+)\\s*-\\s*\\d+\", 1).cast(\"int\")) \\\n       .withColumn(\"visitor_score\", regexp_extract(col(\"score\"), r\"\\d+\\s*-\\s*(\\d+)\", 1).cast(\"int\"))\n\n# Verify the extracted home_score and visitor_score\ndf.select(\"score\", \"home_score\", \"visitor_score\").show(5, truncate\u003dFalse)\n\n# Aggregate total points scored by player in each game (assuming points are from home or visitor team)\nplayer_game_scores \u003d df.groupBy(\"player1_id\", \"game_id\") \\\n                       .agg(\n                           _sum(when(col(\"home_score\") \u003e\u003d 40, col(\"home_score\")).otherwise(0)).alias(\"total_home_points\"),\n                           _sum(when(col(\"visitor_score\") \u003e\u003d 40, col(\"visitor_score\")).otherwise(0)).alias(\"total_visitor_points\")\n                       )\n\n# Calculate the total points for each player by summing both home and visitor points\nplayer_game_scores \u003d player_game_scores.withColumn(\n    \"total_points\", col(\"total_home_points\") + col(\"total_visitor_points\")\n)\n\n# Filter players who scored 40+ points in a game\nplayers_40_plus \u003d player_game_scores.filter(col(\"total_points\") \u003e\u003d 40).select(\"player1_id\").distinct()\n\n# Calculate the total number of players\ntotal_players \u003d df.select(\"player1_id\").distinct().count()\n\n# Calculate percentage of players scoring 40+ points\npercentage_40_plus \u003d (players_40_plus.count() / total_players) * 100\n\nprint(f\"Percentage of players scoring 40+ points: {percentage_40_plus:.2f}%\")"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import when, countDistinct\n\n# Determine the winning team\ndf \u003d df.withColumn(\"winner_team\", when(col(\"home_score\") \u003e col(\"visitor_score\"), col(\"player1_team_name\"))\n                                  .otherwise(col(\"player2_team_name\")))\n\n# Determine the losing team\ndf \u003d df.withColumn(\"loser_team\", when(col(\"home_score\") \u003c col(\"visitor_score\"), col(\"player1_team_name\"))\n                                  .otherwise(col(\"player2_team_name\")))\n\n# Group by loser_team and count distinct game IDs\nmatches_lost \u003d df.groupBy(\"loser_team\").agg(countDistinct(\"game_id\").alias(\"matches_lost\"))\nmatches_lost.show()"
    }
  ]
}